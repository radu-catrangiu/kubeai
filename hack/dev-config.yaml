secretNames:
  huggingface: huggingface

modelServers:
  VLLM:
    images:
      # The key is the image name (referenced from resourceProfiles) and the value is the image.
      # The "default" image should always be specified.
      # "default" is used when no imageName is specified or if a specific image is not found.
      default: "vllm/vllm-openai:v0.6.2"
      cpu: "substratusai/vllm:v0.6.1-cpu"
      nvidia-gpu: "vllm/vllm-openai:v0.6.2"
      google-tpu: "substratusai/vllm:v0.6.1-tpu"
  OLlama:
    images:
      default: "ollama/ollama:latest"
  FasterWhisper:
    images:
      default: "fedirz/faster-whisper-server:latest-cpu"
      nvidia-gpu: "fedirz/faster-whisper-server:latest-cuda"
  Infinity:
    images:
      default: "michaelf34/infinity:latest"

modelDownloaders:
  huggingface:
    image: "us-central1-docker.pkg.dev/substratus-dev/default/huggingface-model-downloader:v0.0.1"

modelRollouts:
  surge: 0
messaging:
  errorMaxBackoff: 30s
  streams: []
  #- requestsURL: gcppubsub://projects/substratus-dev/subscriptions/test-kubeai-requests-sub
  #  responsesURL: gcppubsub://projects/substratus-dev/topics/test-kubeai-responses
  #  maxHandlers: 1
resourceProfiles:
  cpu:
    imageName: "cpu"
    requests:
      # Kind
      cpu: 0.5
      memory: 1Gi
      # GKE
      # cpu: 3
      # memory: 12Gi
    limits:
      cpu: 3
      memory: 12Gi
  nvidia-gpu-l4:
    limits:
      nvidia.com/gpu: "1"
    requests:
      nvidia.com/gpu: "1"
      cpu: "6"
      memory: "24Gi"
  gpu:
    imageName: "nvidia-gpu"
    limits:
      nvidia.com/gpu: "1"
    requests:
      nvidia.com/gpu: "1"
      cpu: "2"
      memory: "8Gi"
    nodeSelector:
      nvidia.com/gpu.product: "NVIDIA-GeForce-RTX-4090"
    tolerations:
      - key: "nvidia.com/gpu"
        operator: "Equal"
        value: "present"
        effect: "NoSchedule"

cacheProfiles:
  fstore:
    sharedFilesystem:
      #storageClassName: "kubeai-filestore"
      storageClassName: "cephfs-ssd-1"
  podlevel:
    modelFilesystem:
      storageClassName: "cephfs-ssd-1"

# Dev-only configuration.
allowPodAddressOverride: true
fixedSelfMetricAddrs: ["127.0.0.1:8080"]

modelAutoscaling:
  interval: 10s
  timeWindow: 60s
  stateConfigMapName: kubeai-autoscaler-state

modelLoaders:
  huggingface:
    image: "substratusai/huggingface-model-loader:v0.9.0"

modelEvaluators:
  huggingface:
    image: "ghcr.io/radu-catrangiu/llm-size-service:latest"
    devProxyHost: "127.0.0.1"
